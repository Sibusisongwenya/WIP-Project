{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPML3dHEQMvcb8PIXLMISc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibusisongwenya/WIP-Project/blob/main/reporting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils/reporting.py\n",
        "\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "def calculate_and_write_metrics(f_report, method_name, preds, labels, metrics, uncerts=None, conf_metrics=None, calibration_curve_data=None):\n",
        "    \"\"\"\n",
        "    Calculates performance metrics and writes them to a report file.\n",
        "\n",
        "    This function writes accuracy, binary metrics, uncertainty metrics, and calibration metrics.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure predictions and labels are squeezed (if possible)\n",
        "        preds = preds.squeeze() if isinstance(preds, np.ndarray) and preds.ndim > 1 else preds\n",
        "        labels = labels.squeeze() if isinstance(labels, np.ndarray) and labels.ndim > 1 else labels\n",
        "\n",
        "        # Accuracy\n",
        "        accuracy = np.mean(preds == labels)\n",
        "        metrics[f'{method_name}_accuracy'] = accuracy\n",
        "        f_report.write(f\"{method_name} Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "        # Binary Metrics (example: converting based on threshold)\n",
        "        binary_labels = np.array([1 if label <= 1 else 0 for label in labels])\n",
        "        binary_preds = np.array([1 if pred <= 1 else 0 for pred in preds])\n",
        "        if len(np.unique(binary_labels)) >= 2:\n",
        "            from sklearn.metrics import roc_auc_score, f1_score\n",
        "            auc = roc_auc_score(binary_labels, binary_preds)\n",
        "            metrics[f'{method_name}_auc'] = auc\n",
        "            f_report.write(f\"{method_name} AUC: {auc:.4f}\\n\")\n",
        "            binary_acc = np.mean(binary_labels == binary_preds)\n",
        "            metrics[f'{method_name}_binary_accuracy'] = binary_acc\n",
        "            f_report.write(f\"{method_name} Binary Accuracy: {binary_acc:.4f}\\n\")\n",
        "            f1 = f1_score(binary_labels, binary_preds)\n",
        "            metrics[f'{method_name}_f1'] = f1\n",
        "            f_report.write(f\"{method_name} F1 Score: {f1:.4f}\\n\")\n",
        "\n",
        "        # Uncertainty Metrics\n",
        "        if uncerts is not None:\n",
        "            avg_uncertainty = np.mean(uncerts)\n",
        "            metrics[f'{method_name}_avg_uncertainty'] = avg_uncertainty\n",
        "            f_report.write(f\"{method_name} Avg Uncertainty: {avg_uncertainty:.4f}\\n\")\n",
        "\n",
        "        # Confidence Metrics (if provided)\n",
        "        if conf_metrics:\n",
        "            for conf, name in conf_metrics:\n",
        "                avg_conf = np.mean(conf)\n",
        "                metrics[f'{method_name}_avg_confidence_{name.lower()}'] = avg_conf\n",
        "                f_report.write(f\"{method_name} Avg Confidence ({name}): {avg_conf:.4f}\\n\")\n",
        "\n",
        "        # Calibration Metrics (example using UCE)\n",
        "        if calibration_curve_data is not None:\n",
        "            from utils.evaluation import calculate_uce\n",
        "            uce_val, _ = calculate_uce(labels, preds, uncerts)\n",
        "            metrics[f'{method_name}_uce'] = uce_val\n",
        "            f_report.write(f\"{method_name} UCE: {uce_val:.2f}%\\n\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calculating metrics for {method_name}: {e}\")\n",
        "\n",
        "\n",
        "def write_patient_predictions(f, model_name, predictions, idx, true_label, class_labels):\n",
        "    \"\"\"\n",
        "    Writes patient predictions for deterministic models.\n",
        "    \"\"\"\n",
        "    if predictions is not None and idx < len(predictions):\n",
        "        pred_class = predictions[idx]\n",
        "        f.write(f\"\\n{model_name} Model Prediction:\\n\")\n",
        "        f.write(f\"  - Predicted Class: {pred_class} ({class_labels[pred_class]})\\n\")\n",
        "        f.write(f\"  - Correct: {'Yes' if pred_class == true_label else 'No'}\\n\")\n",
        "\n",
        "\n",
        "def write_bayesian_predictions(f, model_name, predictions, idx, true_label, class_labels):\n",
        "    \"\"\"\n",
        "    Writes patient predictions for Bayesian models, including uncertainties.\n",
        "    \"\"\"\n",
        "    if predictions is not None and idx < len(predictions[0]):\n",
        "        try:\n",
        "            pred_class = predictions[0][idx]\n",
        "            uncert = predictions[1][idx]\n",
        "            f.write(f\"\\n{model_name} Bayesian Model:\\n\")\n",
        "            f.write(f\"  - Predicted Class: {pred_class} ({class_labels[pred_class]})\\n\")\n",
        "            f.write(f\"  - Uncertainty: {uncert:.2f}\\n\")\n",
        "        except Exception as e:\n",
        "            f.write(f\"\\nError writing {model_name} predictions: {e}\\n\")\n",
        "\n",
        "\n",
        "def generate_patient_case_studies(test_loader, predictions_deterministic=None,\n",
        "                                  predictions_bayesian_llsvi=None,\n",
        "                                  predictions_bayesian_lldropout=None,\n",
        "                                  labels=None, output_dir='output', max_patients=5, class_labels=None):\n",
        "    \"\"\"\n",
        "    Generates patient-level case studies and saves them to a file.\n",
        "\n",
        "    Args:\n",
        "        test_loader: DataLoader for test samples.\n",
        "        predictions_deterministic: Deterministic model predictions.\n",
        "        predictions_bayesian_llsvi: Bayesian LL-SVI predictions tuple.\n",
        "        predictions_bayesian_lldropout: Bayesian LL-Dropout predictions tuple.\n",
        "        labels: Ground truth labels.\n",
        "        output_dir: Directory to save the case studies.\n",
        "        max_patients (int): Maximum number of patient cases to include.\n",
        "        class_labels: Optional list of class names.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    patient_report_file = os.path.join(output_dir, \"patient_case_studies.txt\")\n",
        "    logging.info(f\"Generating patient case studies: {patient_report_file}\")\n",
        "    if class_labels is None:\n",
        "        class_labels = [f\"Class {i}\" for i in range(len(set(labels)))]\n",
        "    with open(patient_report_file, 'w') as f:\n",
        "        f.write(\"Patient Case Studies\\n\" + \"=\" * 50 + \"\\n\")\n",
        "        try:\n",
        "            data_iter = iter(test_loader)\n",
        "            patient_count = 0\n",
        "            while patient_count < max_patients:\n",
        "                try:\n",
        "                    batch = next(data_iter)\n",
        "                except StopIteration:\n",
        "                    break\n",
        "                if batch is None:\n",
        "                    continue\n",
        "                images, true_labels = batch\n",
        "                for i in range(len(true_labels)):\n",
        "                    if patient_count >= max_patients:\n",
        "                        break\n",
        "                    true_label = true_labels[i].item()\n",
        "                    patient_id = f\"Patient_Test_{patient_count+1:03d}\"\n",
        "                    f.write(f\"\\n--- Patient Case Study {patient_count+1} ---\\n\")\n",
        "                    f.write(f\"Patient ID: {patient_id}\\n\")\n",
        "                    f.write(f\"Ground Truth: {true_label} ({class_labels[true_label]})\\n\")\n",
        "                    write_patient_predictions(f, \"Deterministic\", predictions_deterministic, i, true_label, class_labels)\n",
        "                    write_bayesian_predictions(f, \"LL-SVI\", predictions_bayesian_llsvi, i, true_label, class_labels)\n",
        "                    write_bayesian_predictions(f, \"LL-Dropout\", predictions_bayesian_lldropout, i, true_label, class_labels)\n",
        "                    patient_count += 1\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error generating patient case studies: {e}\")\n",
        "    logging.info(f\"Patient case studies saved to: {patient_report_file}\")\n",
        "\n",
        "\n",
        "def generate_clinical_report(overall_metrics: dict, patient_summary: dict, output_dir: str = 'output', filename: str = 'clinical_report.txt'):\n",
        "    \"\"\"\n",
        "    Generates a combined clinical report that includes overall model metrics and a detailed patient-level clinical summary.\n",
        "\n",
        "    overall_metrics: Dictionary with keys for each model variant (e.g., \"Deterministic\", \"Bayesian LL-SVI\", \"Bayesian LL-Dropout\").\n",
        "                     Each key maps to a dictionary of metrics, including:\n",
        "                      - accuracy, auc, binary_accuracy, f1, mae, rmse,\n",
        "                      - prob_conf (Probability-based Confidence),\n",
        "                      - entropy_conf (Entropy-based Confidence),\n",
        "                      - ci_conf (CI-based Confidence),\n",
        "                      - uc (Uncertainty Degree),\n",
        "                      - uce (Expected Uncertainty Calibration Error)\n",
        "\n",
        "    patient_summary: Dictionary with keys:\n",
        "                     - \"patient_id\": str, e.g., \"Patient 042\"\n",
        "                     - \"predicted_score\": float (continuous prediction)\n",
        "                     - \"std\": float (uncertainty)\n",
        "                     - \"ground_truth\": float (continuous ground truth)\n",
        "                     - \"reg_error\": float (|prediction - ground_truth|)\n",
        "                     - \"cdf_probs\": list of 4 floats (for Mayo 0, 1, 2, 3)\n",
        "                     - \"four_class_label\": int (0-3)\n",
        "                     - \"binary_label\": int (0 for remission, 1 for non-remission)\n",
        "                     - \"prob_conf\": float, probability-based confidence (fraction)\n",
        "                     - \"entropy_conf\": float, entropy-based confidence (fraction)\n",
        "                     - \"ci_conf\": float, CI-based confidence (fraction)\n",
        "                     - \"uc\": float, uncertainty degree (percentage)\n",
        "                     - \"uce\": float, Expected Uncertainty Calibration Error (percentage)\n",
        "\n",
        "    The report is written to a text file.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    report_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Compute 95% confidence interval from predicted_score and std\n",
        "    mean = patient_summary.get(\"predicted_score\", 0)\n",
        "    std = patient_summary.get(\"std\", 0)\n",
        "    ci_lower = mean - 1.96 * std\n",
        "    ci_upper = mean + 1.96 * std\n",
        "\n",
        "    with open(report_path, 'w') as f:\n",
        "        # Header\n",
        "        f.write(\"Clinical Report for DenseNet121\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        # Overall Metrics Section\n",
        "        f.write(\"Overall Model Metrics:\\n\\n\")\n",
        "        for model_name, metrics in overall_metrics.items():\n",
        "            f.write(f\"--- {model_name} Model Metrics ---\\n\")\n",
        "            f.write(f\"Accuracy: {metrics.get('accuracy', 0)*100:.1f}%\\n\")\n",
        "            f.write(f\"AUC: {metrics.get('auc', 0):.4f}\\n\")\n",
        "            f.write(f\"Binary Accuracy: {metrics.get('binary_accuracy', 0)*100:.1f}%\\n\")\n",
        "            f.write(f\"F1 Score: {metrics.get('f1', 0):.4f}\\n\")\n",
        "            if 'mae' in metrics:\n",
        "                f.write(f\"MAE: {metrics.get('mae', 0):.4f}\\n\")\n",
        "            if 'rmse' in metrics:\n",
        "                f.write(f\"RMSE: {metrics.get('rmse', 0):.4f}\\n\")\n",
        "            f.write(f\"Probability-based Confidence (Max Probability): {metrics.get('prob_conf', 0)*100:.1f}%\\n\")\n",
        "            f.write(f\"Entropy-based Confidence: {metrics.get('entropy_conf', 0)*100:.1f}%\\n\")\n",
        "            f.write(f\"CI-based Confidence: {metrics.get('ci_conf', 0)*100:.1f}%\\n\")\n",
        "            f.write(f\"Uncertainty Degree (UC): {metrics.get('uc', 0):.1f}%\\n\")\n",
        "            f.write(f\"UCE (Expected Uncertainty Calibration Error): {metrics.get('uce', 0):.1f}%\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        # Patient-Level Summary Section\n",
        "        f.write(\"Patient-Level Case Study\\n\\n\")\n",
        "        patient_id = patient_summary.get(\"patient_id\", \"Patient\")\n",
        "        f.write(f\"{patient_id} Clinical Summary:\\n\")\n",
        "        f.write(f\"Predicted Mayo Score: {mean:.1f} ± {std:.1f}\\n\")\n",
        "        f.write(f\"Ground Truth: {patient_summary.get('ground_truth', 'N/A'):.1f}\\n\")\n",
        "        f.write(f\"Regression Error: {patient_summary.get('reg_error', 'N/A'):.4f}\\n\")\n",
        "        f.write(\"Class Probability Distribution (4-Class):\\n\")\n",
        "        cdf_probs = patient_summary.get(\"cdf_probs\", [0, 0, 0, 0])\n",
        "        f.write(f\"Mayo 0: {cdf_probs[0]*100:.1f}%\\n\")\n",
        "        f.write(f\"Mayo 1: {cdf_probs[1]*100:.1f}%\\n\")\n",
        "        f.write(f\"Mayo 2: {cdf_probs[2]*100:.1f}%\\n\")\n",
        "        f.write(f\"Mayo 3: {cdf_probs[3]*100:.1f}%\\n\")\n",
        "        f.write(f\"95% Confidence Range: [{ci_lower:.1f}, {ci_upper:.1f}]\\n\")\n",
        "        f.write(\"\\nDiscretized Predictions:\\n\")\n",
        "        f.write(f\"Four-Class Label: {patient_summary.get('four_class_label', 'N/A')}\\n\")\n",
        "        f.write(f\"Binary Label: {patient_summary.get('binary_label', 'N/A')} (0 = Remission, 1 = Non-Remission)\\n\")\n",
        "        f.write(\"\\nConfidence Metrics:\\n\")\n",
        "        f.write(f\"Probability-based Confidence (Max Probability): {patient_summary.get('prob_conf', 0)*100:.1f}%\\n\")\n",
        "        f.write(f\"Entropy-based Confidence: {patient_summary.get('entropy_conf', 0)*100:.1f}%\\n\")\n",
        "        f.write(f\"CI-based Confidence: {patient_summary.get('ci_conf', 0)*100:.1f}%\\n\")\n",
        "        f.write(f\"Uncertainty Degree (UC): {patient_summary.get('uc', 0):.1f}%\\n\")\n",
        "        f.write(f\"UCE (Expected Uncertainty Calibration Error): {patient_summary.get('uce', 0):.1f}%\\n\")\n",
        "\n",
        "    logging.info(f\"Clinical report generated and saved to {report_path}\")\n",
        "\n",
        "\n",
        "def generate_patient_case_studies_per_class(test_loader, predictions_deterministic, predictions_bayesian_llsvi, predictions_bayesian_lldropout, output_dir='output', max_patients=4, class_labels=None):\n",
        "    \"\"\"\n",
        "    Generates patient case studies ensuring one sample is included for each of the 4 Mayo classes.\n",
        "    Saves the results to a file.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    patient_report_file = os.path.join(output_dir, \"patient_case_studies_per_class.txt\")\n",
        "    logging.info(f\"Generating patient case studies per class: {patient_report_file}\")\n",
        "\n",
        "    selected_indices = {}\n",
        "    all_labels = predictions_deterministic.get('labels')\n",
        "    if all_labels is None:\n",
        "        logging.error(\"No labels found in deterministic results.\")\n",
        "        return\n",
        "\n",
        "    for idx, label in enumerate(all_labels):\n",
        "        label_int = int(label)\n",
        "        if label_int not in selected_indices and label_int in [0, 1, 2, 3]:\n",
        "            selected_indices[label_int] = idx\n",
        "        if len(selected_indices) >= 4:\n",
        "            break\n",
        "\n",
        "    if class_labels is None:\n",
        "        class_labels = [f\"Mayo {i}\" for i in range(4)]\n",
        "\n",
        "    with open(patient_report_file, 'w') as f:\n",
        "        f.write(\"Patient Case Studies (One Sample Per Mayo Class)\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\")\n",
        "        for cls in sorted(selected_indices.keys()):\n",
        "            idx = selected_indices[cls]\n",
        "            cont_pred = predictions_bayesian_llsvi['continuous_predictions'][idx]\n",
        "            std = predictions_bayesian_llsvi['uncertainties'][idx]\n",
        "            ground_truth = all_labels[idx]\n",
        "            reg_error = abs(cont_pred - ground_truth)\n",
        "            cdf_probs = predictions_bayesian_llsvi['cdf_probs'][idx]\n",
        "            four_class_label = predictions_bayesian_llsvi['four_class_predictions'][idx]\n",
        "            binary_label = predictions_bayesian_llsvi['binary_predictions'][idx]\n",
        "            prob_conf = np.max(cdf_probs)\n",
        "            # Import helper functions for confidence metrics\n",
        "            from utils.helpers import entropy_confidence, ci_confidence, uncertainty_degree\n",
        "            entropy_conf_val = np.mean(entropy_confidence(np.expand_dims(cdf_probs, axis=0)))\n",
        "            ci_conf_val = np.mean(ci_confidence(np.array([std])))\n",
        "            uc = np.mean(uncertainty_degree(np.expand_dims(cdf_probs, axis=0)))\n",
        "            from utils.evaluation import calculate_uce\n",
        "            uce, _ = calculate_uce(np.array([ground_truth]), np.array([cont_pred]), np.array([std]))\n",
        "\n",
        "            f.write(f\"\\n--- Patient Case Study for {class_labels[cls]} ---\\n\")\n",
        "            f.write(f\"Patient Index: {idx}\\n\")\n",
        "            f.write(f\"Ground Truth: {ground_truth} ({class_labels[cls]})\\n\")\n",
        "            f.write(f\"Predicted Mayo Score: {cont_pred:.1f} ± {std:.1f}\\n\")\n",
        "            f.write(f\"Regression Error: {reg_error:.4f}\\n\")\n",
        "            f.write(\"Class Probability Distribution (4-Class):\\n\")\n",
        "            f.write(f\"  Mayo 0: {cdf_probs[0]*100:.1f}%\\n\")\n",
        "            f.write(f\"  Mayo 1: {cdf_probs[1]*100:.1f}%\\n\")\n",
        "            f.write(f\"  Mayo 2: {cdf_probs[2]*100:.1f}%\\n\")\n",
        "            f.write(f\"  Mayo 3: {cdf_probs[3]*100:.1f}%\\n\")\n",
        "            ci_lower = cont_pred - 1.96 * std\n",
        "            ci_upper = cont_pred + 1.96 * std\n",
        "            f.write(f\"95% Confidence Range: [{ci_lower:.1f}, {ci_upper:.1f}]\\n\")\n",
        "            f.write(\"\\nDiscretized Predictions:\\n\")\n",
        "            f.write(f\"  Four-Class Label: {four_class_label}\\n\")\n",
        "            f.write(f\"  Binary Label: {binary_label} (0 = Remission, 1 = Non-Remission)\\n\")\n",
        "            f.write(\"\\nConfidence Metrics:\\n\")\n",
        "            f.write(f\"  Probability-based Confidence (Max Probability): {prob_conf*100:.1f}%\\n\")\n",
        "            f.write(f\"  Entropy-based Confidence: {entropy_conf_val*100:.1f}%\\n\")\n",
        "            f.write(f\"  CI-based Confidence: {ci_conf_val*100:.1f}%\\n\")\n",
        "            f.write(f\"  Uncertainty Degree (UC): {uc:.1f}%\\n\")\n",
        "            f.write(f\"  UCE (Expected Uncertainty Calibration Error): {uce:.1f}%\\n\")\n",
        "\n",
        "    logging.info(f\"Patient case studies generated and saved to: {patient_report_file}\")\n"
      ],
      "metadata": {
        "id": "S31Dcu5zzwF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}