{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrGJe9fHtj10tqdUVvhDyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibusisongwenya/WIP-Project/blob/main/magic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "plZDCem_bHYh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision.models import densenet121, DenseNet121_Weights\n",
        "import logging\n",
        "from typing import Optional\n",
        "from collections import OrderedDict\n",
        "from torchbnn.modules import BayesLinear\n",
        "\n",
        "\n",
        "class BayesianDenseNet121_LLSVI(nn.Module):\n",
        "    \"\"\"\n",
        "    Bayesian DenseNet121 model using LLSVI in classifier layers for regression.\n",
        "    Outputs a single continuous value.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained: bool = True):\n",
        "        super(BayesianDenseNet121_LLSVI, self).__init__()\n",
        "        # Initialize the DenseNet121 model with or without pretrained weights.\n",
        "        self.densenet121 = models.densenet121(pretrained=pretrained)\n",
        "\n",
        "        # Replace the original classifier with a Bayesian version.\n",
        "\n",
        "        # Here we assume BayesLinear is a Bayesian equivalent of nn.Linear.\n",
        "        in_features = self.densenet121.classifier.in_features\n",
        "        out_features = 1  # Assuming you want a single output neuron\n",
        "        self.densenet121.classifier = nn.Sequential(\n",
        "            BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=in_features, out_features=out_features), # Corrected instantiation\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, sample: Optional[bool] = False) -> torch.Tensor:\n",
        "        # Forward pass through the modified DenseNet121.\n",
        "        return self.densenet121(x)\n",
        "\n",
        "\n",
        "# In the BayesianDenseNet121_LLSVI class\n",
        "        self.classifier = nn.Sequential(\n",
        "            BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=num_features, out_features=128), # specify prior and size\n",
        "            nn.SiLU(),\n",
        "            BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=128, out_features=1) # specify prior and size\n",
        "        )\n",
        "\n",
        "\n",
        "def init_weights(self) -> None:\n",
        "    \"\"\"\n",
        "    Initializes the weights of the model using appropriate distributions.\n",
        "    \"\"\"\n",
        "    for m in self.modules():\n",
        "        if isinstance(m, BayesLinear):\n",
        "            # The following lines assume the layer has `W_mu` and `W_rho`\n",
        "            nn.init.kaiming_normal_(m.W_mu, mode='fan_out', nonlinearity='relu')  # Initialize W_mu\n",
        "            nn.init.normal_(m.W_rho, 0, 0.1)  # Initialize W_rho with a normal distribution\n",
        "\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, sample: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through DenseNet121 and Bayesian classifier.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "            sample (bool): Flag to enable sampling in BayesLinear layers.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Model output.\n",
        "        \"\"\"\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "\n",
        "        for module in self.classifier:\n",
        "            if isinstance(module, BayesLinear):\n",
        "                out = module(out, sample=sample)\n",
        "            else:\n",
        "                out = module(out)\n",
        "        return out\n",
        "\n",
        "    def kl_loss(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute KL divergence from all BayesLinear layers for regularization.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: KL divergence value.\n",
        "        \"\"\"\n",
        "        kl = 0.0\n",
        "        for m in self.modules():\n",
        "            if hasattr(m, \"kl_loss\"):\n",
        "                kl += m.kl_loss()\n",
        "        return kl\n",
        "\n",
        "\n",
        "class DenseNet121_LLDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    DenseNet121 model with Last Layer Dropout (MC-Dropout) for Bayesian regression.\n",
        "    Outputs a single continuous value and allows uncertainty estimation via MC sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained: bool = True, dropout_prob: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.dropout_prob = dropout_prob\n",
        "        weights = DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        self.densenet = densenet121(weights=weights)\n",
        "\n",
        "        # Add dropout to the features module and classifier\n",
        "        self.densenet.features.add_module(\"dropout1\", nn.Dropout(p=dropout_prob))\n",
        "        num_features = self.densenet.classifier.in_features\n",
        "        self.densenet.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(num_features, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, sample: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass with optional MC-Dropout sampling.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "            sample (bool): If True, forces dropout layers to train mode for MC sampling.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Model output.\n",
        "        \"\"\"\n",
        "        if sample:\n",
        "            # Save original dropout states and force dropout layers to train mode for sampling\n",
        "            dropout_modules = [m for m in self.modules() if isinstance(m, nn.Dropout)]\n",
        "            orig_states = {m: m.training for m in dropout_modules}\n",
        "            for m in dropout_modules:\n",
        "                m.train()\n",
        "\n",
        "            out = self.densenet(x)\n",
        "\n",
        "            # Revert dropout layers to their original states\n",
        "            for m in dropout_modules:\n",
        "                if not orig_states[m]:\n",
        "                    m.eval()\n",
        "            return out\n",
        "        else:\n",
        "            return self.densenet(x)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def load_bayesian_model(checkpoint_path: str, device: torch.device, pretrained: bool = True) -> torch.nn.Module:\n",
        "    \"\"\"\n",
        "    Load the Bayesian DenseNet121_LLSVI regression model from a checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path (str): Path to the model checkpoint file.\n",
        "        device (torch.device): The device to load the model onto.\n",
        "        pretrained (bool): Whether to initialize with pretrained weights.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The loaded Bayesian model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the model without calling init_weights()\n",
        "        model = BayesianDenseNet121_LLSVI(pretrained=pretrained)\n",
        "        logging.info(\"Initialized BayesianDenseNet121_LLSVI model.\")\n",
        "\n",
        "\n",
        "        # Load the checkpoint file from disk\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        # Extract state_dict (if the checkpoint is wrapped in a dict with \"state_dict\")\n",
        "        state_dict = checkpoint[\"state_dict\"] if \"state_dict\" in checkpoint else checkpoint\n",
        "\n",
        "        new_state_dict = OrderedDict()\n",
        "        for old_key, value in state_dict.items():\n",
        "            # If key starts with \"features.\", prepend \"densenet121.\"\n",
        "            if old_key.startswith(\"features.\"):\n",
        "                new_key = \"densenet121.\" + old_key\n",
        "                new_state_dict[new_key] = value\n",
        "\n",
        "            # If key is from the classifier, remap to the Bayesian layer parameters\n",
        "            elif old_key.startswith(\"classifier.\"):\n",
        "                # Map deterministic weight to Bayesian mean and add default log sigma\n",
        "                if old_key == \"classifier.weight\":\n",
        "                    new_state_dict[\"densenet121.classifier.0.weight_mu\"] = value\n",
        "                    # Initialize log sigma to a small constant, e.g., -5.0\n",
        "                    new_state_dict[\"densenet121.classifier.0.weight_log_sigma\"] = torch.full_like(value, -5.0)\n",
        "                # Map deterministic bias to Bayesian mean and add default log sigma\n",
        "                elif old_key == \"classifier.bias\":\n",
        "                    new_state_dict[\"densenet121.classifier.0.bias_mu\"] = value\n",
        "                    new_state_dict[\"densenet121.classifier.0.bias_log_sigma\"] = torch.full_like(value, -5.0)\n",
        "                else:\n",
        "                    # For any other classifier key, prepend \"densenet121.\"\n",
        "                    new_key = \"densenet121.\" + old_key\n",
        "                    new_state_dict[new_key] = value\n",
        "            else:\n",
        "                # For any other key, if not already prefixed, add \"densenet121.\"\n",
        "                if not old_key.startswith(\"densenet121.\"):\n",
        "                    new_key = \"densenet121.\" + old_key\n",
        "                else:\n",
        "                    new_key = old_key\n",
        "                new_state_dict[new_key] = value\n",
        "\n",
        "        # Load the renamed state_dict into the model\n",
        "        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n",
        "        print(\"Missing keys:\", missing_keys)\n",
        "        print(\"Unexpected keys:\", unexpected_keys)\n",
        "        logging.info(\"State dict loaded successfully.\")\n",
        "\n",
        "        # Move model to device\n",
        "        model.to(device)\n",
        "        return model\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load Bayesian model: {e}\")\n",
        "        raise\n",
        "def load_mc_dropout_model(\n",
        "    checkpoint_path: str,\n",
        "    device: torch.device,\n",
        "    pretrained: bool = True,\n",
        "    dropout_prob: float = 0.5\n",
        ") -> torch.nn.Module:\n",
        "    \"\"\"\n",
        "    Load the DenseNet121_LLDropout regression model from a checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path (str): Path to the model checkpoint file.\n",
        "        device (torch.device): The device to load the model onto.\n",
        "        pretrained (bool): Whether to initialize with pretrained weights.\n",
        "        dropout_prob (float): Dropout probability for MC-Dropout layers.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The loaded MC-Dropout model.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Initialize the model\n",
        "        model = DenseNet121_LLDropout(pretrained=pretrained, dropout_prob=dropout_prob)\n",
        "        logging.info(\"Initialized DenseNet121_LLDropout model with dropout probability: %.2f\", dropout_prob)\n",
        "\n",
        "\n",
        "        # 1. Load the checkpoint once\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "        # 2. Extract the actual state_dict\n",
        "        if \"state_dict\" in checkpoint:\n",
        "            state_dict = checkpoint[\"state_dict\"]\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "\n",
        "        new_state_dict = OrderedDict()\n",
        "        for old_key, value in state_dict.items():\n",
        "            # Case 1: If the key starts with \"features.\", prepend \"densenet.\"\n",
        "            if old_key.startswith(\"features.\"):\n",
        "                new_key = \"densenet.\" + old_key\n",
        "\n",
        "            # Case 2: If the key starts with \"classifier.\", rename it to \"densenet.classifier.1\"\n",
        "            elif old_key.startswith(\"classifier.\"):\n",
        "                # e.g., \"classifier.weight\" -> \"classifier.1.weight\"\n",
        "                suffix = old_key[len(\"classifier\"):]  # e.g. \".weight\"\n",
        "                new_key = \"densenet.classifier.1\" + suffix\n",
        "\n",
        "            # Otherwise, keep the key as-is\n",
        "            else:\n",
        "                new_key = old_key\n",
        "\n",
        "            new_state_dict[new_key] = value\n",
        "\n",
        "        # Load with strict=False first to see which keys match or don't match\n",
        "        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n",
        "        print(\"Missing keys:\", missing_keys)\n",
        "        print(\"Unexpected keys:\", unexpected_keys)\n",
        "\n",
        "        # If you want to enforce strict loading after verifying the renamed keys:\n",
        "        # model.load_state_dict(new_state_dict, strict=True)\n",
        "        logging.info(\"State dict loaded successfully.\")\n",
        "\n",
        "        model.to(device)\n",
        "        return model\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load MC-Dropout model: {e}\")\n",
        "        raise"
      ]
    }
  ]
}