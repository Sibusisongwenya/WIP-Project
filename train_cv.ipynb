{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibusisongwenya/WIP-Project/blob/main/train_cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJNNtliSsMWt",
        "outputId": "69118896-01ee-4aca-dea9-24bde4558bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdfN50Mzstce",
        "outputId": "adbb57a8-7668-4c89-ddba-f04bd7f1063c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchbnn\n",
            "  Downloading torchbnn-1.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading torchbnn-1.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: torchbnn\n",
            "Successfully installed torchbnn-1.2\n",
            "Current working directory: /content/drive/MyDrive/uc\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "# Append the project root to the Python path\n",
        "sys.path.append('/content/drive/MyDrive/uc')\n",
        "!pip install torchbnn\n",
        "os.chdir(\"/content/drive/MyDrive/uc\")\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA_iw2pVr3yA",
        "outputId": "29d517e5-82df-475c-9d76-f5e6eed8ef69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 77.4MB/s]\n",
            "<ipython-input-3-6c1a41e2d73a>:149: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_llsvi = GradScaler(enabled=USE_AMP)\n",
            "<ipython-input-3-6c1a41e2d73a>:150: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_dropout = GradScaler(enabled=USE_AMP)\n",
            "<ipython-input-3-6c1a41e2d73a>:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# If you want to use mixed precision (amp), import these:\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Import dataset and model definitions\n",
        "from dataset.ucmayo4 import UCMayo4\n",
        "from utils.magic import BayesianDenseNet121_LLSVI, DenseNet121_LLDropout\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# ----------- GPU optimization flags -----------\n",
        "torch.backends.cudnn.benchmark = True  # Let cuDNN optimize for your input sizes\n",
        "# ----------------------------------------------\n",
        "\n",
        "# ------------------\n",
        "# Hyperparameters\n",
        "# ------------------\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "DROPOUT_PROB = 0.5\n",
        "K_FOLDS = 10\n",
        "CHECKPOINT_DIR = \"weights_cv/\"\n",
        "USE_AMP = True  # Set to False if you don't want to use mixed precision\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Data setup\n",
        "train_dir = \"/content/drive/MyDrive/uc/test_set/train\"\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "full_dataset = UCMayo4(root_dir=train_dir, transform=transform)\n",
        "dataset_size = len(full_dataset)\n",
        "logging.info(f\"Full dataset size: {dataset_size} samples\")\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Metric definitions\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "def compute_mae_rmse(outputs, targets):\n",
        "    diff = outputs - targets\n",
        "    mae = diff.abs().mean().item()\n",
        "    rmse = torch.sqrt((diff ** 2).mean()).item()\n",
        "    return mae, rmse\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scaler, device):\n",
        "    model.train()\n",
        "    running_loss, total_mae, total_rmse = 0.0, 0.0, 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, targets in dataloader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True).float().unsqueeze(1)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if USE_AMP:\n",
        "            # Mixed precision\n",
        "            with autocast():\n",
        "                outputs = model(images, sample=False)\n",
        "                loss = mse_loss(outputs, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(images, sample=False)\n",
        "            loss = mse_loss(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        # track metrics\n",
        "        with torch.no_grad():\n",
        "            mae, rmse = compute_mae_rmse(outputs, targets)\n",
        "            total_mae += mae * batch_size\n",
        "            total_rmse += rmse * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_mae  = total_mae / total_samples\n",
        "    epoch_rmse = total_rmse / total_samples\n",
        "    return epoch_loss, epoch_mae, epoch_rmse\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(model, dataloader, device):\n",
        "    model.eval()\n",
        "    running_loss, total_mae, total_rmse = 0.0, 0.0, 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, targets in dataloader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True).float().unsqueeze(1)\n",
        "\n",
        "        outputs = model(images, sample=False)\n",
        "        loss = mse_loss(outputs, targets)\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        mae, rmse = compute_mae_rmse(outputs, targets)\n",
        "        total_mae += mae * batch_size\n",
        "        total_rmse += rmse * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_mae  = total_mae / total_samples\n",
        "    epoch_rmse = total_rmse / total_samples\n",
        "    return epoch_loss, epoch_mae, epoch_rmse\n",
        "\n",
        "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "indices = np.arange(dataset_size)\n",
        "\n",
        "fold_results_llsvi = []\n",
        "fold_results_dropout = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
        "    logging.info(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    val_subset   = Subset(full_dataset, val_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              pin_memory=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              pin_memory=True, num_workers=2)\n",
        "\n",
        "    # Initialize models\n",
        "    model_llsvi = BayesianDenseNet121_LLSVI(pretrained=True).to(DEVICE)\n",
        "    model_dropout = DenseNet121_LLDropout(pretrained=True, dropout_prob=DROPOUT_PROB).to(DEVICE)\n",
        "\n",
        "    optimizer_llsvi = optim.Adam(model_llsvi.parameters(), lr=LEARNING_RATE)\n",
        "    optimizer_dropout = optim.Adam(model_dropout.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # For mixed precision\n",
        "    scaler_llsvi = GradScaler(enabled=USE_AMP)\n",
        "    scaler_dropout = GradScaler(enabled=USE_AMP)\n",
        "\n",
        "    best_val_loss_llsvi = float('inf')\n",
        "    best_val_loss_dropout = float('inf')\n",
        "    final_val_mae_llsvi, final_val_rmse_llsvi = 0.0, 0.0\n",
        "    final_val_mae_dropout, final_val_rmse_dropout = 0.0, 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        logging.info(f\"Fold {fold+1}, Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        # LLSVI train/val\n",
        "        train_loss_llsvi, train_mae_llsvi, train_rmse_llsvi = train_epoch(\n",
        "            model_llsvi, train_loader, optimizer_llsvi, scaler_llsvi, DEVICE\n",
        "        )\n",
        "        val_loss_llsvi, val_mae_llsvi, val_rmse_llsvi = validate_epoch(\n",
        "            model_llsvi, val_loader, DEVICE\n",
        "        )\n",
        "\n",
        "        # Dropout train/val\n",
        "        train_loss_dropout, train_mae_dropout, train_rmse_dropout = train_epoch(\n",
        "            model_dropout, train_loader, optimizer_dropout, scaler_dropout, DEVICE\n",
        "        )\n",
        "        val_loss_dropout, val_mae_dropout, val_rmse_dropout = validate_epoch(\n",
        "            model_dropout, val_loader, DEVICE\n",
        "        )\n",
        "\n",
        "        logging.info(\n",
        "          f\"LL-SVI    [E{epoch+1}] Train MSE: {train_loss_llsvi:.4f},\"\n",
        "          f\" Val MSE: {val_loss_llsvi:.4f}, MAE: {val_mae_llsvi:.4f}, RMSE: {val_rmse_llsvi:.4f}\"\n",
        "        )\n",
        "        logging.info(\n",
        "          f\"Dropout   [E{epoch+1}] Train MSE: {train_loss_dropout:.4f},\"\n",
        "          f\" Val MSE: {val_loss_dropout:.4f}, MAE: {val_mae_dropout:.4f}, RMSE: {val_rmse_dropout:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Save best fold checkpoint for LLSVI\n",
        "        if val_loss_llsvi < best_val_loss_llsvi:\n",
        "            best_val_loss_llsvi = val_loss_llsvi\n",
        "            final_val_mae_llsvi, final_val_rmse_llsvi = val_mae_llsvi, val_rmse_llsvi\n",
        "            ckpt_path_llsvi = os.path.join(CHECKPOINT_DIR, f\"fold_{fold+1}_best_llsvi.pth\")\n",
        "            torch.save(model_llsvi.state_dict(), ckpt_path_llsvi)\n",
        "\n",
        "        # Save best fold checkpoint for Dropout\n",
        "        if val_loss_dropout < best_val_loss_dropout:\n",
        "            best_val_loss_dropout = val_loss_dropout\n",
        "            final_val_mae_dropout, final_val_rmse_dropout = val_mae_dropout, val_rmse_dropout\n",
        "            ckpt_path_dropout = os.path.join(CHECKPOINT_DIR, f\"fold_{fold+1}_best_dropout.pth\")\n",
        "            torch.save(model_dropout.state_dict(), ckpt_path_dropout)\n",
        "\n",
        "    # Accumulate final results from each fold\n",
        "    fold_results_llsvi.append((best_val_loss_llsvi, final_val_mae_llsvi, final_val_rmse_llsvi))\n",
        "    fold_results_dropout.append((best_val_loss_dropout, final_val_mae_dropout, final_val_rmse_dropout))\n",
        "\n",
        "    # Free up memory, especially if you have big models\n",
        "    del model_llsvi, model_dropout\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Summarize cross-fold results\n",
        "fold_results_llsvi = np.array(fold_results_llsvi)  # shape: (K_FOLDS, 3) => [mse, mae, rmse]\n",
        "fold_results_dropout = np.array(fold_results_dropout)\n",
        "\n",
        "mean_llsvi = np.mean(fold_results_llsvi, axis=0)\n",
        "std_llsvi  = np.std(fold_results_llsvi, axis=0)\n",
        "mean_dropout = np.mean(fold_results_dropout, axis=0)\n",
        "std_dropout  = np.std(fold_results_dropout, axis=0)\n",
        "\n",
        "logging.info(\"\\n=== Cross-Validation Results (LL-SVI) ===\")\n",
        "logging.info(f\"MSE : {mean_llsvi[0]:.4f} +/- {std_llsvi[0]:.4f}\")\n",
        "logging.info(f\"MAE : {mean_llsvi[1]:.4f} +/- {std_llsvi[1]:.4f}\")\n",
        "logging.info(f\"RMSE: {mean_llsvi[2]:.4f} +/- {std_llsvi[2]:.4f}\")\n",
        "\n",
        "logging.info(\"\\n=== Cross-Validation Results (MC-Dropout) ===\")\n",
        "logging.info(f\"MSE : {mean_dropout[0]:.4f} +/- {std_dropout[0]:.4f}\")\n",
        "logging.info(f\"MAE : {mean_dropout[1]:.4f} +/- {std_dropout[1]:.4f}\")\n",
        "logging.info(f\"RMSE: {mean_dropout[2]:.4f} +/- {std_dropout[2]:.4f}\")\n",
        "\n",
        "logging.info(\"Cross-validation complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "26WbdlJgSNl1",
        "outputId": "cc4261cd-545c-49d8-c4d7-aae3351d8b36"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfngwenya\u001b[0m (\u001b[33mfngwenya-z\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/uc/wandb/run-20250317_025819-q21gvhz3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/q21gvhz3' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/q21gvhz3' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/q21gvhz3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 153MB/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold_1/epoch</td><td>▁▃▅▆█</td></tr><tr><td>fold_1/train_loss_dropout</td><td>█▃▂▂▁</td></tr><tr><td>fold_1/train_loss_llsvi</td><td>█▅▂▃▁</td></tr><tr><td>fold_1/train_mae_dropout</td><td>█▃▂▂▁</td></tr><tr><td>fold_1/train_mae_llsvi</td><td>█▄▂▂▁</td></tr><tr><td>fold_1/train_rmse_dropout</td><td>█▃▂▂▁</td></tr><tr><td>fold_1/train_rmse_llsvi</td><td>█▅▃▂▁</td></tr><tr><td>fold_1/val_loss_dropout</td><td>▂█▁▃▅</td></tr><tr><td>fold_1/val_loss_llsvi</td><td>▁▁▁▁█</td></tr><tr><td>fold_1/val_mae_dropout</td><td>▄█▁▃▆</td></tr><tr><td>fold_1/val_mae_llsvi</td><td>▁▁▁▁█</td></tr><tr><td>fold_1/val_rmse_dropout</td><td>▄▅▁▁█</td></tr><tr><td>fold_1/val_rmse_llsvi</td><td>▁▁▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold_1/epoch</td><td>5</td></tr><tr><td>fold_1/train_loss_dropout</td><td>0.23568</td></tr><tr><td>fold_1/train_loss_llsvi</td><td>1.96064</td></tr><tr><td>fold_1/train_mae_dropout</td><td>0.3579</td></tr><tr><td>fold_1/train_mae_llsvi</td><td>1.04187</td></tr><tr><td>fold_1/train_rmse_dropout</td><td>0.47713</td></tr><tr><td>fold_1/train_rmse_llsvi</td><td>1.2908</td></tr><tr><td>fold_1/val_loss_dropout</td><td>0.35094</td></tr><tr><td>fold_1/val_loss_llsvi</td><td>27537406.72531</td></tr><tr><td>fold_1/val_mae_dropout</td><td>0.42365</td></tr><tr><td>fold_1/val_mae_llsvi</td><td>4067.13475</td></tr><tr><td>fold_1/val_rmse_dropout</td><td>0.56809</td></tr><tr><td>fold_1/val_rmse_llsvi</td><td>4439.3975</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/q21gvhz3' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/q21gvhz3</a><br> View project at: <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_025819-q21gvhz3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/uc/wandb/run-20250317_030355-mu9t83jz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/mu9t83jz' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/mu9t83jz' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/mu9t83jz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold_2/epoch</td><td>▁▃▅▆█</td></tr><tr><td>fold_2/train_loss_dropout</td><td>█▄▂▁▁</td></tr><tr><td>fold_2/train_loss_llsvi</td><td>█▃▃▁▃</td></tr><tr><td>fold_2/train_mae_dropout</td><td>█▄▂▁▁</td></tr><tr><td>fold_2/train_mae_llsvi</td><td>█▂▃▁▃</td></tr><tr><td>fold_2/train_rmse_dropout</td><td>█▄▂▁▁</td></tr><tr><td>fold_2/train_rmse_llsvi</td><td>█▃▃▁▃</td></tr><tr><td>fold_2/val_loss_dropout</td><td>▃▂▁▃█</td></tr><tr><td>fold_2/val_loss_llsvi</td><td>▁▂▁▁█</td></tr><tr><td>fold_2/val_mae_dropout</td><td>▃▂▁▄█</td></tr><tr><td>fold_2/val_mae_llsvi</td><td>▂▇▂▁█</td></tr><tr><td>fold_2/val_rmse_dropout</td><td>▃▂▁▃█</td></tr><tr><td>fold_2/val_rmse_llsvi</td><td>▁▅▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold_2/epoch</td><td>5</td></tr><tr><td>fold_2/train_loss_dropout</td><td>0.25384</td></tr><tr><td>fold_2/train_loss_llsvi</td><td>2.66822</td></tr><tr><td>fold_2/train_mae_dropout</td><td>0.37198</td></tr><tr><td>fold_2/train_mae_llsvi</td><td>1.22963</td></tr><tr><td>fold_2/train_rmse_dropout</td><td>0.49455</td></tr><tr><td>fold_2/train_rmse_llsvi</td><td>1.50317</td></tr><tr><td>fold_2/val_loss_dropout</td><td>0.46405</td></tr><tr><td>fold_2/val_loss_llsvi</td><td>85.97657</td></tr><tr><td>fold_2/val_mae_dropout</td><td>0.46707</td></tr><tr><td>fold_2/val_mae_llsvi</td><td>2.1487</td></tr><tr><td>fold_2/val_rmse_dropout</td><td>0.65883</td></tr><tr><td>fold_2/val_rmse_llsvi</td><td>5.54147</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_2</strong> at: <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/mu9t83jz' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/mu9t83jz</a><br> View project at: <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_030355-mu9t83jz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/uc/wandb/run-20250317_030932-r9l6l3ko</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/r9l6l3ko' target=\"_blank\">fold_3</a></strong> to <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/r9l6l3ko' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/r9l6l3ko</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fold_3/epoch</td><td>▁▃▅▆█</td></tr><tr><td>fold_3/train_loss_dropout</td><td>█▃▂▁▁</td></tr><tr><td>fold_3/train_loss_llsvi</td><td>█▅▃▃▁</td></tr><tr><td>fold_3/train_mae_dropout</td><td>█▄▂▁▁</td></tr><tr><td>fold_3/train_mae_llsvi</td><td>█▅▃▄▁</td></tr><tr><td>fold_3/train_rmse_dropout</td><td>█▄▂▁▁</td></tr><tr><td>fold_3/train_rmse_llsvi</td><td>█▅▃▄▁</td></tr><tr><td>fold_3/val_loss_dropout</td><td>▂▁▁█▂</td></tr><tr><td>fold_3/val_loss_llsvi</td><td>▂▁▁█▁</td></tr><tr><td>fold_3/val_mae_dropout</td><td>▅▂▁█▃</td></tr><tr><td>fold_3/val_mae_llsvi</td><td>▂▂▁█▁</td></tr><tr><td>fold_3/val_rmse_dropout</td><td>▆▁▁█▂</td></tr><tr><td>fold_3/val_rmse_llsvi</td><td>▂▂▁█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fold_3/epoch</td><td>5</td></tr><tr><td>fold_3/train_loss_dropout</td><td>0.24415</td></tr><tr><td>fold_3/train_loss_llsvi</td><td>2.36609</td></tr><tr><td>fold_3/train_mae_dropout</td><td>0.36553</td></tr><tr><td>fold_3/train_mae_llsvi</td><td>1.16331</td></tr><tr><td>fold_3/train_rmse_dropout</td><td>0.48506</td></tr><tr><td>fold_3/train_rmse_llsvi</td><td>1.42328</td></tr><tr><td>fold_3/val_loss_dropout</td><td>0.22877</td></tr><tr><td>fold_3/val_loss_llsvi</td><td>1.216</td></tr><tr><td>fold_3/val_mae_dropout</td><td>0.36838</td></tr><tr><td>fold_3/val_mae_llsvi</td><td>0.87329</td></tr><tr><td>fold_3/val_rmse_dropout</td><td>0.4558</td></tr><tr><td>fold_3/val_rmse_llsvi</td><td>0.95907</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_3</strong> at: <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/r9l6l3ko' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/r9l6l3ko</a><br> View project at: <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_030932-r9l6l3ko/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/uc/wandb/run-20250317_031508-dyyvp4my</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/dyyvp4my' target=\"_blank\">fold_4</a></strong> to <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/dyyvp4my' target=\"_blank\">https://wandb.ai/fngwenya-z/my-ucmayo4-cv/runs/dyyvp4my</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# WandB import\n",
        "import wandb\n",
        "\n",
        "from dataset.ucmayo4 import UCMayo4\n",
        "from utils.magic import BayesianDenseNet121_LLSVI, DenseNet121_LLDropout\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# ------------------\n",
        "# Hyperparameters\n",
        "# ------------------\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "DROPOUT_PROB = 0.5\n",
        "K_FOLDS = 10\n",
        "CHECKPOINT_DIR = \"weights_cv/\"\n",
        "PROJECT_NAME = \"my-ucmayo4-cv\"  # Name for your wandb project\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Data setup\n",
        "train_dir = \"/content/drive/MyDrive/uc/test_set/train\"\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "full_dataset = UCMayo4(root_dir=train_dir, transform=transform)\n",
        "dataset_size = len(full_dataset)\n",
        "logging.info(f\"Full dataset size: {dataset_size} samples\")\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Define MSE plus additional metrics\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "def compute_mae_rmse(outputs, targets):\n",
        "    diff = outputs - targets\n",
        "    mae = diff.abs().mean().item()\n",
        "    rmse = torch.sqrt((diff**2).mean()).item()\n",
        "    return mae, rmse\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, total_samples = 0.0, 0\n",
        "    total_mae, total_rmse = 0.0, 0.0\n",
        "\n",
        "    for images, targets in dataloader:\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device).float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, sample=False)  # normal forward\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        mae_batch, rmse_batch = compute_mae_rmse(outputs, targets)\n",
        "        total_mae += mae_batch * batch_size\n",
        "        total_rmse += rmse_batch * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_mae  = total_mae / total_samples\n",
        "    epoch_rmse = total_rmse / total_samples\n",
        "    return epoch_loss, epoch_mae, epoch_rmse\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, total_samples = 0.0, 0\n",
        "    total_mae, total_rmse = 0.0, 0.0\n",
        "\n",
        "    for images, targets in dataloader:\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device).float().unsqueeze(1)\n",
        "        outputs = model(images, sample=False)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        mae_batch, rmse_batch = compute_mae_rmse(outputs, targets)\n",
        "        total_mae += mae_batch * batch_size\n",
        "        total_rmse += rmse_batch * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_mae  = total_mae / total_samples\n",
        "    epoch_rmse = total_rmse / total_samples\n",
        "    return epoch_loss, epoch_mae, epoch_rmse\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "indices = np.arange(dataset_size)\n",
        "\n",
        "fold_results_llsvi = []\n",
        "fold_results_dropout = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
        "    logging.info(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
        "\n",
        "    # Initialize a new wandb run for each fold\n",
        "    wandb_run = wandb.init(\n",
        "        project=PROJECT_NAME,\n",
        "        name=f\"fold_{fold+1}\",\n",
        "        config={\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"learning_rate\": LEARNING_RATE,\n",
        "            \"dropout_prob\": DROPOUT_PROB,\n",
        "            \"fold_index\": fold+1\n",
        "        },\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    val_subset   = Subset(full_dataset, val_idx)\n",
        "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Initialize models\n",
        "    model_llsvi = BayesianDenseNet121_LLSVI(pretrained=True).to(DEVICE)\n",
        "    model_dropout = DenseNet121_LLDropout(pretrained=True, dropout_prob=DROPOUT_PROB).to(DEVICE)\n",
        "\n",
        "    optimizer_llsvi = optim.Adam(model_llsvi.parameters(), lr=LEARNING_RATE)\n",
        "    optimizer_dropout = optim.Adam(model_dropout.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    best_val_loss_llsvi = float('inf')\n",
        "    best_val_loss_dropout = float('inf')\n",
        "    final_val_mae_llsvi, final_val_rmse_llsvi = 0.0, 0.0\n",
        "    final_val_mae_dropout, final_val_rmse_dropout = 0.0, 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        logging.info(f\"Fold {fold+1}, Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        # Train LLSVI\n",
        "        train_loss_llsvi, train_mae_llsvi, train_rmse_llsvi = train_epoch(\n",
        "            model_llsvi, train_loader, mse_loss, optimizer_llsvi, DEVICE\n",
        "        )\n",
        "        # Validate LLSVI\n",
        "        val_loss_llsvi, val_mae_llsvi, val_rmse_llsvi = validate_epoch(\n",
        "            model_llsvi, val_loader, mse_loss, DEVICE\n",
        "        )\n",
        "\n",
        "        # Train MC-Dropout\n",
        "        train_loss_dropout, train_mae_dropout, train_rmse_dropout = train_epoch(\n",
        "            model_dropout, train_loader, mse_loss, optimizer_dropout, DEVICE\n",
        "        )\n",
        "        # Validate MC-Dropout\n",
        "        val_loss_dropout, val_mae_dropout, val_rmse_dropout = validate_epoch(\n",
        "            model_dropout, val_loader, mse_loss, DEVICE\n",
        "        )\n",
        "\n",
        "        # Log metrics to wandb\n",
        "        wandb.log({\n",
        "            f\"fold_{fold+1}/epoch\": epoch+1,\n",
        "\n",
        "            # LLSVI\n",
        "            f\"fold_{fold+1}/train_loss_llsvi\": train_loss_llsvi,\n",
        "            f\"fold_{fold+1}/val_loss_llsvi\": val_loss_llsvi,\n",
        "            f\"fold_{fold+1}/train_mae_llsvi\": train_mae_llsvi,\n",
        "            f\"fold_{fold+1}/val_mae_llsvi\": val_mae_llsvi,\n",
        "            f\"fold_{fold+1}/train_rmse_llsvi\": train_rmse_llsvi,\n",
        "            f\"fold_{fold+1}/val_rmse_llsvi\": val_rmse_llsvi,\n",
        "\n",
        "            # Dropout\n",
        "            f\"fold_{fold+1}/train_loss_dropout\": train_loss_dropout,\n",
        "            f\"fold_{fold+1}/val_loss_dropout\": val_loss_dropout,\n",
        "            f\"fold_{fold+1}/train_mae_dropout\": train_mae_dropout,\n",
        "            f\"fold_{fold+1}/val_mae_dropout\": val_mae_dropout,\n",
        "            f\"fold_{fold+1}/train_rmse_dropout\": train_rmse_dropout,\n",
        "            f\"fold_{fold+1}/val_rmse_dropout\": val_rmse_dropout\n",
        "        }, step=epoch+1)\n",
        "\n",
        "        logging.info(\n",
        "          f\"[LL-SVI: fold {fold+1}, epoch {epoch+1}] \"\n",
        "          f\"Train MSE: {train_loss_llsvi:.4f}, Val MSE: {val_loss_llsvi:.4f}, \"\n",
        "          f\"MAE: {val_mae_llsvi:.4f}, RMSE: {val_rmse_llsvi:.4f}\"\n",
        "        )\n",
        "        logging.info(\n",
        "          f\"[Dropout: fold {fold+1}, epoch {epoch+1}] \"\n",
        "          f\"Train MSE: {train_loss_dropout:.4f}, Val MSE: {val_loss_dropout:.4f}, \"\n",
        "          f\"MAE: {val_mae_dropout:.4f}, RMSE: {val_rmse_dropout:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Save best fold checkpoint for LLSVI\n",
        "        if val_loss_llsvi < best_val_loss_llsvi:\n",
        "            best_val_loss_llsvi = val_loss_llsvi\n",
        "            final_val_mae_llsvi, final_val_rmse_llsvi = val_mae_llsvi, val_rmse_llsvi\n",
        "            torch.save(model_llsvi.state_dict(), os.path.join(CHECKPOINT_DIR, f\"fold_{fold+1}_best_llsvi.pth\"))\n",
        "\n",
        "        # Save best fold checkpoint for Dropout\n",
        "        if val_loss_dropout < best_val_loss_dropout:\n",
        "            best_val_loss_dropout = val_loss_dropout\n",
        "            final_val_mae_dropout, final_val_rmse_dropout = val_mae_dropout, val_rmse_dropout\n",
        "            torch.save(model_dropout.state_dict(), os.path.join(CHECKPOINT_DIR, f\"fold_{fold+1}_best_dropout.pth\"))\n",
        "\n",
        "    # Store final results for each model in this fold\n",
        "    fold_results_llsvi.append((best_val_loss_llsvi, final_val_mae_llsvi, final_val_rmse_llsvi))\n",
        "    fold_results_dropout.append((best_val_loss_dropout, final_val_mae_dropout, final_val_rmse_dropout))\n",
        "\n",
        "    # End wandb run for this fold\n",
        "    wandb_run.finish()\n",
        "\n",
        "    # Clean up memory\n",
        "    del model_llsvi, model_dropout\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Summarize cross-fold results\n",
        "fold_results_llsvi = np.array(fold_results_llsvi)  # shape (K_FOLDS, 3) => columns = [val_mse, val_mae, val_rmse]\n",
        "fold_results_dropout = np.array(fold_results_dropout)\n",
        "\n",
        "mean_llsvi = np.mean(fold_results_llsvi, axis=0)\n",
        "std_llsvi  = np.std(fold_results_llsvi, axis=0)\n",
        "mean_dropout = np.mean(fold_results_dropout, axis=0)\n",
        "std_dropout  = np.std(fold_results_dropout, axis=0)\n",
        "\n",
        "logging.info(\"\\n=== Cross-Validation Results (LL-SVI) ===\")\n",
        "logging.info(f\"MSE : {mean_llsvi[0]:.4f} +/- {std_llsvi[0]:.4f}\")\n",
        "logging.info(f\"MAE : {mean_llsvi[1]:.4f} +/- {std_llsvi[1]:.4f}\")\n",
        "logging.info(f\"RMSE: {mean_llsvi[2]:.4f} +/- {std_llsvi[2]:.4f}\")\n",
        "\n",
        "logging.info(\"\\n=== Cross-Validation Results (MC-Dropout) ===\")\n",
        "logging.info(f\"MSE : {mean_dropout[0]:.4f} +/- {std_dropout[0]:.4f}\")\n",
        "logging.info(f\"MAE : {mean_dropout[1]:.4f} +/- {std_dropout[1]:.4f}\")\n",
        "logging.info(f\"RMSE: {mean_dropout[2]:.4f} +/- {std_dropout[2]:.4f}\")\n",
        "\n",
        "logging.info(\"Cross-validation complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOT9cQcfUX1aECdTKb/IDwd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}