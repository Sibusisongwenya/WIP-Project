{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKqtZVkEs01EVpHQS65wTM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibusisongwenya/WIP-Project/blob/main/tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnC-69ci9Ihu"
      },
      "outputs": [],
      "source": [
        "# utils/tuning.py\n",
        "\n",
        "import itertools\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "def tune_hyperparameters(train_loader, val_loader, param_grids, tuning_method, bayesian_model_class, output_dir, class_weighted_loss, device):\n",
        "    \"\"\"\n",
        "    Performs hyperparameter tuning for Bayesian models.\n",
        "\n",
        "    Args:\n",
        "        train_loader: Training DataLoader.\n",
        "        val_loader: Validation DataLoader.\n",
        "        param_grids (dict): Dictionary of parameters to try.\n",
        "        tuning_method (str): 'grid' for exhaustive grid search or 'random' for random search.\n",
        "        bayesian_model_class: The Bayesian model class to instantiate.\n",
        "        output_dir (str): Directory to save tuning results.\n",
        "        class_weighted_loss (bool): Whether to use class weighted loss.\n",
        "        device (torch.device): The computation device.\n",
        "\n",
        "    Returns:\n",
        "        dict: The best configuration found.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting hyperparameter tuning...\")\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generate parameter combinations\n",
        "    if tuning_method == 'grid':\n",
        "        keys = list(param_grids.keys())\n",
        "        combinations = list(itertools.product(*param_grids.values()))\n",
        "        param_combinations = [dict(zip(keys, comb)) for comb in combinations]\n",
        "    else:\n",
        "        # Random search: sample N times (N=5 for example)\n",
        "        N = 5\n",
        "        param_combinations = []\n",
        "        for _ in range(N):\n",
        "            config = {key: np.random.choice(values) for key, values in param_grids.items()}\n",
        "            param_combinations.append(config)\n",
        "\n",
        "    for params in param_combinations:\n",
        "        logging.info(f\"Testing configuration: {params}\")\n",
        "        try:\n",
        "            # Instantiate the model with parameters (ensure bayesian_model_class accepts these as keyword args)\n",
        "            model = bayesian_model_class(num_classes=4, pretrained=True, **params).to(device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=params.get('lr', 1e-3))\n",
        "            criterion = torch.nn.CrossEntropyLoss()\n",
        "            # train_model should be defined elsewhere; here we assume it returns (trained_model, val_loss)\n",
        "            trained_model, val_loss = train_model(model, train_loader, val_loader, epochs=params.get('epochs', 5),\n",
        "                                                  bayesian=True, class_weighted_loss=class_weighted_loss)\n",
        "            results.append({'config': params, 'val_loss': val_loss})\n",
        "            logging.info(f\"Configuration {params} resulted in val_loss: {val_loss:.4f}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error with configuration {params}: {e}\")\n",
        "\n",
        "    # Save results\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results.to_csv(f\"{output_dir}/tuning_results_{bayesian_model_class.__name__}.csv\", index=False)\n",
        "    logging.info(\"Hyperparameter tuning complete.\")\n",
        "    best_config = min(results, key=lambda x: x['val_loss'])\n",
        "    logging.info(f\"Best configuration: {best_config}\")\n",
        "    return best_config\n"
      ]
    }
  ]
}